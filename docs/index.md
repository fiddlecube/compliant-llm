# Prompt Secure Documentation

Welcome to the Prompt Secure documentation. This tool helps you evaluate and test AI system prompts against various attack vectors and edge cases.

## Overview

Prompt Secure is designed to help AI developers and security researchers test the robustness of their system prompts by simulating various attack strategies such as prompt injection, jailbreaking, adversarial inputs, and more.

## Getting Started

- [Installation](installation.md)
- [Quick Start Guide](quickstart.md)
- [CLI Documentation](cli/index.md)
- [Configuration Guide](configuration/index.md)
- [Example Use Cases](examples/index.md)
- [API Reference](reference/index.md)

## Features

- Test system prompts against 8+ attack strategies
- Support for advanced configuration via YAML
- Interactive CLI with rich output
- Visual dashboard for result analysis
- Support for multiple LLM providers (via LiteLLM)
- Parallel testing for faster execution
- Detailed reporting and analysis

## License

This project is licensed under [LICENSE] - see the LICENSE file for details.

## Contributing

Contributions are welcome! Please see our [Contributing Guidelines](contributing.md) for more information.
